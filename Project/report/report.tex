\documentclass[a4paper]{scrartcl}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{pgfplots}
\usepackage{amsmath, enumerate, amssymb, multirow, fancyhdr, color, graphicx, lastpage, listings, tikz, pdflscape, subfigure, float, polynom, hyperref, tabularx, forloop, geometry, listings, fancybox, tikz, forest, tabstackengine, cancel, multicol, algorithm, diagbox}
\usepackage[noend]{algpseudocode}
\input kvmacros
\geometry{a4paper,left=3cm, right=3cm, top=3cm, bottom=3cm}
\pagestyle{fancy}
\title{Final project for the Course High Performance Programming, Philipp Noel von Bachmann}


\begin{document}
\maketitle
\section{Introduction}
    Machine Learning and Data analysis are some of the most influencial fields
    in our current society. One of the main task in Machine Learning is to give
    a prediction based on some input variables. A common algorithm to use is
    linear-least-square, which finds the best fit for a linear model. Here, we
    need an efficient way to calculate this fit. In Data analysis, a common tool
    is the Principal Component analysis, which tries to reduce the
    dimensionality of the data in a way, that the reconstruction error gets
    minimized. PCA relies on an eigenvalue decomposition. For both methods, we
    can use the QR decomposition too speed up the computation.


\section{Problem description}
    Suppose $A \in \mathbb{R}^{m\times n}$ is a real, square matrix. Then it can be shown that $A$ can be
    decomposed into 
    \begin{equation}\label{eq:QR}
        A = QR
    \end{equation}
    where $Q$ is orthogonal and R is upper triangular. Finding $Q \in \mathbb{R}^{m\times m}$ and $R \in \mathbb{R}^{m\times n}$ is
    the task of the algorithm.

\section{Solution}
    We will implement an algorithm known as \textbf{Givens Rotation}. This
    algorithm relies on construction a sequences of matrices $G_i$, such that
    when multiplying $A$ with $G_i$, we get a new matrix with a zero at a
    predefined place. Choosing $G_i$ such that we eliminate the lower diagonal
    of A, we end up with a $R$ and by multiplying all $G_i$ with $Q$. We will
    first show how to eliminate one value at the time by constructing $G_i$ and
    then how to combine these.

    \subsection{Givens rotation}
        First, we define a Givens rotation matrix as 
        \begin{equation}\label{eq:Givens}
            G(i,j,\theta) = \begin{bmatrix} 
                1   &   &   \\
                    &   \ddots&   \\
                    &   & c_{ii} & \cdots& -s_{ij}\\
                    &   & \vdots & \ddots & \vdots\\
                    &   & s_{ji} & \cdots & c_{jj}\\
                    &   &  & & &\ddots\\
                    &   &  & & &&1\\            
                \end{bmatrix}
        \end{equation}
        where $c=\cos(\theta)$, $s=\sin(\theta)$ and any not filled out values are 0.
        Note: Therefore we can equally represent $G$ by $G(i,j,c,s)$
    
    \subsection{Eliminating one value}
        We will how to find $\theta$ to solve
        \begin{equation}
            G(i,j,\theta)^T \begin{bmatrix} 
                \times\\
                \vdots\\
                a\\
                \vdots\\
                b\\
                \vdots\\
                \times
                \end{bmatrix}
            = \begin{bmatrix} 
                \times\\
                \vdots\\
                r\\
                \vdots\\
                0\\
                \vdots\\
                \times
                \end{bmatrix}
        \end{equation}
        where $\times$ are arbitrary numbers $a,b \in \mathbb{R}$, $r=\sqrt{a^2+b^2}$. 
        A trivial solution would be 
        % \begin{align}
        %     c = \frac{a}{r} & s = \frac{-b}{r}
        % \end{align}.
        \begin{multicols}{2}
            \begin{equation}
                c = \frac{a}{r}
            \end{equation}\break
            \begin{equation}
                s = \frac{b}{r}
            \end{equation}
          \end{multicols}
        However, $r$ is prone to overflow, so we can instead store it in a
        different way. If
        \begin{itemize}
            \item $\lvert b \rvert \geq \lvert a \rvert$:\\
                $t=\frac{a}{b}$, $s=\frac{sgn(b)}{\sqrt{1+t^2}}, c=st$
            \item else:\\
                $t=\frac{b}{a}$, $c=\frac{sgn(a)}{\sqrt{1+t^2}}, s=ct$
        \end{itemize}

    \subsection{Order of elimination}\label{sub:ElimOrder}

    Let us assume that we have
    $R= \begin{bmatrix}
        x & x & x \\
        a=0 & x & x \\
        b=0 & c & x \\
    \end{bmatrix}$, which we get after finishing the first column in
    algorithm \ref{alg:QR_Givens}. The next steps would be to set $c=0$.
    This in turn modifies row 1,2 in $R$ afterwards, which means $a$ and
    $b$ necessarly need to be 0 in order to not destroy the solution.
    Therefore, if we want to set $R_{i,j}$ to 0, then we need to make
    sure that $R_{i, :j}$ and $R_{i-1, :j}$ are already 0.

    \subsection{Final algorithm}
        \begin{algorithm}[H]
            \caption{QR factorization with Givens rotation}\label{alg:QR_Givens}
            \begin{algorithmic}[1]
            \Procedure{Factorize}{$A$}
            \State set $R=A$, $Q=I$
            \For{$j$ in 1 to $n$}
                \For{$i$ in $m$ down to $j+1$}
                    \State compute the Givens rotation $G(i,i-1,c,s)$ eliminating $R_{ij}$ with $a=R_{i-1,j}$, $b=R_{i-1,j}$.
                    \State set $R=G(i,j,c,s) R$, $Q = Q G(i,j,c,s)$
                \EndFor
            \EndFor
            \EndProcedure
            \end{algorithmic}
        \end{algorithm}

        Note that we have to do it in this order, since each update affects row and row above.

\section{Experiments}
    \subsection{Check for correctness}
    \subsection{Results}
        \subsubsection{Format}
            In the following section, we will show timing results for different
            matrix sizes. We denote the sizes by $i,j$, which we defined as
            decomposing the matrix $A \in \mathbb{R}^{i\times j}$.

        \subsubsection{Baseline Results}
            \noindent\begin{tabular}{c|c|c|c}
                \backslashbox{$i$}{$j$} & 10 & 25 & 50 \\
                \hline
                10 & 0.0005 & 0.0008 & 0.001 \\
                25 & 0.018  & 0.034 & 0.049 \\
                50 & 0.235 & 0.611 & 1.076 \\
            \end{tabular}\\[10pt]

            We see that the if we vary $i$, we get a superlinear increase of
            runtime. If we increase $j$ instead, we see that we get a nearly
            linear increase in runtime.
            
            (We should only get a minor increase in runtime for j, since we dont
            need to calculate new 0s). (For i this makes sense, since increasing
            i by one adds i new 0s to compute.)

        \subsection{Optimizing the matmul}
            If we take a closer look at the Givens rotation matrices, we see
            that they are very sparse. Additionally, we know in advance which
            entries of the givens rotation wil be nonzero. Thus we can optimize
            our matmul. First of all, we can only recalculate columns that will
            get modified by the multiplication. If we multiply $GR$, where
            $G=G(i,j,c,s)$, then we only need to modify rows $i,j$ in $R$.
            Secondly, we know that for calculating those rows we only need to
            muliply by the $c,s$ in the corresponding row of $G$, since all
            other values are 0.

            We need further distinguish between left- ($G R$) and right-hand
            ($QG$) multiplication. We show the pseudo-code for left-hand, but
            right-hand is very similar.

            \begin{algorithm}[H]
                \caption{optimized left-hand Matmul with Givens}\label{alg:step}
                \begin{algorithmic}[1]
                \Procedure{Matmul}{$G=G(i,j,c,s)$, $R$}
                    \State copy the two rows i,j of R into temporary array $T = \begin{bmatrix}
                        R_{i, :} \\
                        R_{j, :} \\
                    \end{bmatrix}$

                \For{$k$ in columns of $R$}
                    \State $R_{i, k} = c \cdot T_{1, k} - s \cdot T_{2, k}$
                    \State $R_{j, k} = s \cdot T_{1, k} + c \cdot T_{2, k}$
                \EndFor
                \Return $R$
                \EndProcedure
                \end{algorithmic}
            \end{algorithm}

            This leads to the following results:\\
            \noindent\begin{tabular}{c|c|c|c}
                \backslashbox{$i$}{$j$} & 10 & 25 & 50 \\
                \hline
                50 & 0.0003 & 0.0007 & 0.0011 \\
                500 & 0.019 & 0.040 & 0.079 \\
            \end{tabular}\\[10pt]

            We see that for $i=50,j=50$ we get a runtime of roughly
            $\frac{1}{1000}$ of the original runtime. However these new runtimes
            vary quite a bit and we would need to scale them up to get more
            stable, which is not feasible for the baseline.

        \subsection{Compiler flags}
            For i,j=500
            \noindent\begin{tabular}{c|c|c|c|c}
                Baseline & -O1 & -O2 & -O3 & -03 -march=native -ffast-math\\
                \hline
                0.747 & 0.167 & 0.137 & 0.134 & 0.119\\
            \end{tabular}\\[10pt]

        \subsection{Further serial optimizations}
            Tried those further optimizations
            \begin{itemize}
                \item Write pow calls out (compiler flag probably does it already)
                \item Exchanged div in computation of $c,s$ by multiplication with $\frac{1}{r}$
                \item Declare the array sizes as constant
            \end{itemize}

            Those ones helped:
            \begin{itemize}
                \item Inline functions, took time down from 1.09 to 0.99 for i,j=1000
            \end{itemize}

        \subsection{Cache blocking}
            Doesnt really make sense, since we dont have any matmul anymore. 
            For loading of q and r, they are already optimal since we take the two closest cchelines for our mul






            \begin{itemize}
                \item optimize the representation of G, thus also optimizing matmul of G
                \item cache usage (mayby useful?)
            \end{itemize}

        \subsection{serial code optimization}
            \begin{itemize}
                \item maybe profile code
                \item DONE use compiler flags, best gcc -O3 -march=native -ffast-math
                \item DONE look if we can reduce arithmetic, eg no div in loop
                \item DONE inline functions
                \item loop unrolling
                \item no if statements
                \item change pow calls
                \item cache blocking
                \item make constants, especially i and j
                \item pure functions
                \item uautovectorization (included in 03)
            \end{itemize}


        \subsection{Parallelization}
            \subsubsection{Theory and Implementation}
                In order to parallelize our code, we first need to check which
                parts can be done in parallel. First, note that the computation
                of the Givens rotation in line 5 of algorithm
                \ref{alg:QR_Givens} is a pure function, eg just depends on the
                inputs and doesn't have any side effects. Thus, we only need to
                focus on updating $Q,R$ in line 6. Here, it is necessary to
                distinguish in two cases:
                \begin{itemize}
                    \item For left-hand Givens rotation $GR$, if $G=G(i,j,c,s)$,
                    only rows $i,j$ of $R$ get modified
                    \item \item For righ-hand Givens rotation $QG$, if
                    $G=G(i,j,c,s)$, only columns $i,j$ of $Q$ get modified
                \end{itemize}
                Consequently, two update steps rotating around
                $G(i_1,j_1,c_1,s_1)$ and $G(i_2,j_2,c_2,s_2)$ are independent,
                if $i_1, i_2, j_1, j_2$ are pairwise distinct. This means we
                need to make sure that if we set $R_{i,j}$ to 0 in one
                iteration, we can set any other elements in Row $i,j$ to 0 in
                the same iteration, in order to not interfere between the
                computations.
                
                
                In combination with the constrain that if we want to set
                $R_{i,j}$ to 0, $R_{i, :j}$ and $R_{i-1, :j}$ already need to be
                0, (see Section \ref{sub:ElimOrder}) we arrive at the following
                scheme, shown for a $5\times 5$ matrix:
                \begin{equation}\label{eq:It_Scheme}
                    \begin{bmatrix}
                        x & x & x & x & x \\
                        3 & x & x & x & x \\
                        2 & 4 & x & x & x \\
                        1 & 3 & 5 & x & x \\
                        0 & 2 & 4 & 6 & x \\
                    \end{bmatrix}
                \end{equation}
                where the values denote in which iteration we can set the
                element to 0 without cross-interference. This shows that we can
                set $R_{2,0}$ and $R_{4,1}$ to 0 in parallel for example.

                We arrive at the parallel algorithm:
                \begin{algorithm}[H]
                    \caption{parallel QR factorization }\label{alg:QR_Parallel}
                    \begin{algorithmic}[1]
                        \Procedure{Factorize\_Parallel}{$A \in \mathbb{R}^{m\times n}$}
                            \State set $R=A$, $Q=I$
                            \For{$it$ in 1 to $(n-1)*2+m-n$}
                                \State In parallel:
                                \For{$i,j$ given by the iteration scheme \ref{eq:It_Scheme}}
                                    \State compute the Givens rotation $G(i,i-1,c,s)$ eliminating $R_{ij}$ 
                                    \State set $R=G(i,j,c,s) R$, $Q = Q G(i,j,c,s)$
                                \EndFor
                            \EndFor
                        \EndProcedure
                    \end{algorithmic}
                \end{algorithm}

                The code was getting parallelized with \textbf{OpenMP}. Since we
                have equal load in line 6+7 of algorithm \ref{alg:QR_Parallel},
                we don't need to use load balancing.

                Looking at \ref{eq:It_Scheme}, we cannot expect a linear scaling
                with the number of threads. For example in iteration 0 and 1, we
                only have 1 element so there is no speedup by using more
                threads. More formally if $A \in \mathbb{R}^{m\times n}$, then
                we can have a maximum of $\min(m/2, n)$ computations running in
                parallel, achieved in the itertation where we set $R_{1,0}$ to
                0.

            \subsubsection{Results}
                Because the computation of $i,j$ is a bit more involved now, we
                get a slightly higher baseline with 1.137s compared to 0.986s
                for our serial code and $i,j=1000$. However when increasing the number of threads:\\
                \begin{tikzpicture}[scale=0.7]
                    \begin{axis}[
                        xmin = 0, xmax = 21,
                        ymin = 0.4, ymax = 1.2,
                        xtick distance = 1,
                        ytick distance = 0.1,
                        grid = both,
                        minor tick num = 1,
                        major grid style = {lightgray},
                        minor grid style = {lightgray!25},
                        width = 0.8\textwidth,
                        height = 0.8\textwidth,
                        xlabel = {Number of threads},
                        ylabel = {Time (s)}]
                    
                    
                    % Plot data from a file
                    \addplot[
                        scatter,
                        thin,
                        red,
                        dashed
                    ] file[skip first] {data/parallel.dat};
                    
                    \end{axis}
                \end{tikzpicture}

            \noindent\begin{tabular}{c|c|c|c|c|c}
                 &10 & 100 & 1000 & 2000 & 4000\\
                \hline
                $j=1000, i=$ & 0.031 & 0.023 & 0.425 & 4.125 & 58.866 \\
                $i=1000, j=$ & 0.031 & 0.087 & 0.430 & 1.084 & 2.247\\
            \end{tabular}\\[10pt]

            


            


\section{Memory optimization}
    Maybe try to store Q in column format and R in row format??



\section{Random} 
    \begin{itemize}
        \item we can order them by affecting row and above
        \item best is probably to create tasks because upper rows require less work
        \item maybe need memory optimizations, but most of the operations seem to be near anyway
    \end{itemize}

    TODO:
    \begin{itemize}
        \item implement checks
        \item implement good storage of G
        \item implement efficient matmul of G
        \item implement one givens rotation
        \item implement outer loop
        \item serial optimizations
        \item parallelizations, probably with openmp
    \end{itemize}





\section{Experiments}
\section{Conclusion}
\section{References}
$https://www.math.usm.edu/lambers/mat610/sum10/lecture9.pdf
https://www.math.usm.edu/lambers/mat610/class0208.pdf
https://en.wikipedia.org/wiki/Givens_rotation$





\end{document}

