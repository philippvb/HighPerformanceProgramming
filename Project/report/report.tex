\documentclass[a4paper]{scrartcl}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{pgfplots}
\usepackage{amsmath, enumerate, amssymb, multirow, fancyhdr, color, graphicx, lastpage, listings, tikz, pdflscape, subfigure, float, polynom, hyperref, tabularx, forloop, geometry, listings, fancybox, tikz, forest, tabstackengine, cancel, multicol, algorithm, diagbox}
\usepackage[noend]{algpseudocode}
\input kvmacros
\geometry{a4paper,left=3cm, right=3cm, top=3cm, bottom=3cm}
\pagestyle{fancy}
\title{Final project for the Course High Performance Programming, Philipp Noel von Bachmann}


\begin{document}
\maketitle
\section{Introduction}
    Machine Learning and Data analysis are some of the most influencial fields
    in our current society. One of the main task in Machine Learning is to give
    a prediction based on some input variables. A common algorithm to use is
    linear-least-square, which finds the best fit for a linear model. Here, we
    need an efficient way to calculate this fit. In Data analysis, a common tool
    is the Principal Component analysis, which tries to reduce the
    dimensionality of the data in a way, that the reconstruction error gets
    minimized. PCA relies on an eigenvalue decomposition. For both methods, we
    can use the QR decomposition too speed up the computation.


\section{Problem description}
    Suppose $A$ is a real, square matrix. Then it can be shown that $A$ can be
    decomposed into 
    \begin{equation}\label{eq:QR}
        A = QR
    \end{equation}
    where $Q$ is orthogonal and R is upper triangular. Finding $Q$ and $R$ is
    the task of the algorithm.

\section{Solution}
    We will implement an algorithm known as \textbf{Givens Rotation}. This
    algorithm relies on construction a sequences of matrices $G_i$, such that
    when multiplying $A$ with $G_i$, we get a new matrix with a zero at a
    predefined place. Choosing $G_i$ such that we eliminate the lower diagonal
    of A, we end up with a $R$ and by multiplying all $G_i$ with $Q$. We will
    first show how to eliminate one value at the time by constructing $G_i$ and
    then how to combine these.

    \subsection{Givens rotation}
        First, we define a Givens rotation matrix as 
        \begin{equation}\label{eq:Givens}
            G(i,j,\theta) = \begin{bmatrix} 
                1   &   &   \\
                    &   \ddots&   \\
                    &   & c_{ii} & \cdots& -s_{ij}\\
                    &   & \vdots & \ddots & \vdots\\
                    &   & s_{ji} & \cdots & c_{jj}\\
                    &   &  & & &\ddots\\
                    &   &  & & &&1\\            
                \end{bmatrix}
        \end{equation}
        where $c=\cos(\theta)$, $s=\sin(\theta)$ and any not filled out values are 0.
        Note: Therefore we can equally represent $G$ by $G(i,j,c,s)$
    
    \subsection{Eliminating one value}
        We will how to find $\theta$ to solve
        \begin{equation}
            G(i,j,\theta)^T \begin{bmatrix} 
                \times\\
                \vdots\\
                a\\
                \vdots\\
                b\\
                \vdots\\
                \times
                \end{bmatrix}
            = \begin{bmatrix} 
                \times\\
                \vdots\\
                r\\
                \vdots\\
                0\\
                \vdots\\
                \times
                \end{bmatrix}
        \end{equation}
        where $\times$ are arbitrary numbers $a,b \in \mathbb{R}$, $r=\sqrt{a^2+b^2}$. 
        A trivial solution would be 
        % \begin{align}
        %     c = \frac{a}{r} & s = \frac{-b}{r}
        % \end{align}.
        \begin{multicols}{2}
            \begin{equation}
                c = \frac{a}{r}
            \end{equation}\break
            \begin{equation}
                s = \frac{b}{r}
            \end{equation}
          \end{multicols}
        However, $r$ is prone to overflow, so we can instead store it in a
        different way. If
        \begin{itemize}
            \item $\lvert b \rvert \geq \lvert a \rvert$:\\
                $t=\frac{a}{b}$, $s=\frac{sgn(b)}{\sqrt{1+t^2}}, c=st$
            \item else:\\
                $t=\frac{b}{a}$, $c=\frac{sgn(a)}{\sqrt{1+t^2}}, s=ct$
        \end{itemize}

    \subsection{Final algorithm}
        \begin{algorithm}[H]
            \caption{Givens rotation}\label{alg:step}
            \begin{algorithmic}[1]
            \Procedure{Step}{}
            \State set $R=A$, $Q=I$
            \For{$j$ in 1 to $n$}
                \For{$i$ in $n$ down to $j+1$}
                    \State compute the Givens rotation $G(i,j,c,s)$ eliminating $R_{ij}$ with $a=i$, $b=i-1$.
                    \State set $R=G(i,j,c,s) R$, $Q = Q G(i,j,c,s)$
                    \State
                \EndFor
            \EndFor
            \EndProcedure
            \end{algorithmic}
        \end{algorithm}

        Note that we have to do it in this order, since each update affects row and row above.

\section{Experiments}
    \subsection{Check for correctness}
    \subsection{Results}
        \subsubsection{Format}
            In the following section, we will show timing results for different
            matrix sizes. We denote the sizes by $i,j$, which we defined as
            decomposing the matrix $A \in \mathbb{R}^{i\times j}$.

        \subsubsection{Baseline Results}
            \noindent\begin{tabular}{c|c|c|c}
                \backslashbox{$i$}{$j$} & 10 & 25 & 50 \\
                \hline
                10 & 0.0005 & 0.0008 & 0.001 \\
                25 & 0.018  & 0.034 & 0.049 \\
                50 & 0.235 & 0.611 & 1.076 \\
            \end{tabular}\\[10pt]

            We see that the if we vary $i$, we get a superlinear increase of
            runtime. If we increase $j$ instead, we see that we get a nearly
            linear increase in runtime.
            
            (We should only get a minor increase in runtime for j, since we dont
            need to calculate new 0s). (For i this makes sense, since increasing
            i by one adds i new 0s to compute.)

        \subsection{Optimizing the matmul}
            If we take a closer look at the Givens rotation matrices, we see
            that they are very sparse. Additionally, we know in advance which
            entries of the givens rotation wil be nonzero. Thus we can optimize
            our matmul. First of all, we can only recalculate columns that will
            get modified by the multiplication. If we multiply $GR$, where
            $G=G(i,j,c,s)$, then we only need to modify rows $i,j$ in $R$.
            Secondly, we know that for calculating those rows we only need to
            muliply by the $c,s$ in the corresponding row of $G$, since all
            other values are 0.

            We need further distinguish between left- ($G R$) and right-hand
            ($QG$) multiplication. We show the pseudo-code for left-hand, but
            right-hand is very similar.

            \begin{algorithm}[H]
                \caption{optimized left-hand Matmul with Givens}\label{alg:step}
                \begin{algorithmic}[1]
                \Procedure{Matmul}{$G=G(i,j,c,s)$, $R$}
                    \State copy the two rows i,j of R into temporary array $T = \begin{bmatrix}
                        R_{i, :} \\
                        R_{j, :} \\
                    \end{bmatrix}$

                \For{$k$ in columns of $R$}
                    \State $R_{i, k} = c \cdot T_{1, k} - s \cdot T_{2, k}$
                    \State $R_{j, k} = s \cdot T_{1, k} + c \cdot T_{2, k}$
                \EndFor
                \Return $R$
                \EndProcedure
                \end{algorithmic}
            \end{algorithm}

            This leads to the following results:\\
            \noindent\begin{tabular}{c|c|c|c}
                \backslashbox{$i$}{$j$} & 10 & 25 & 50 \\
                \hline
                50 & 0.0003 & 0.0007 & 0.0011 \\
                500 & 0.019 & 0.040 & 0.079 \\
            \end{tabular}\\[10pt]

            We see that for $i=50,j=50$ we get a runtime of roughly
            $\frac{1}{1000}$ of the original runtime. However these new runtimes
            vary quite a bit and we would need to scale them up to get more
            stable, which is not feasible for the baseline.

        \subsection{Compiler flags}
            For i,j=500
            \noindent\begin{tabular}{c|c|c|c|c}
                Baseline & -O1 & -O2 & -O3 & -03 -march=native -ffast-math\\
                \hline
                0.747 & 0.167 & 0.137 & 0.134 & 0.119\\
            \end{tabular}\\[10pt]

        \subsection{Further serial optimizations}
            Tried those further optimizations
            \begin{itemize}
                \item Write pow calls out (compiler flag probably does it already)
                \item Exchanged div in computation of $c,s$ by multiplication with $\frac{1}{r}$
                \item Declare the array sizes as constant
            \end{itemize}

            Those ones helped:
            \begin{itemize}
                \item Inline functions, took time down from 1.09 to 0.99 for i,j=1000
            \end{itemize}

        \subsection{Cache blocking}
            Doesnt really make sense, since we dont have any matmul anymore. 
            For loading of q and r, they are already optimal since we take the two closest cchelines for our mul






            \begin{itemize}
                \item optimize the representation of G, thus also optimizing matmul of G
                \item cache usage (mayby useful?)
            \end{itemize}

        \subsection{serial code optimization}
            \begin{itemize}
                \item maybe profile code
                \item DONE use compiler flags, best gcc -O3 -march=native -ffast-math
                \item DONE look if we can reduce arithmetic, eg no div in loop
                \item DONE inline functions
                \item loop unrolling
                \item no if statements
                \item change pow calls
                \item cache blocking
                \item make constants, especially i and j
                \item pure functions
                \item uautovectorization (included in 03)
            \end{itemize}


        \subsection{Parallelization}
            We can modify multiple lines as same time, however need to make sure that not the two same. ideas:
            Go down by two, then with offset of one again.
            Create tasks since these tasks might take different amount of time.
            Maybe try with locking instead of the choosing rows, however probably not that successfull.


\section{Random} 
    \begin{itemize}
        \item we can order them by affecting row and above
        \item best is probably to create tasks because upper rows require less work
        \item maybe need memory optimizations, but most of the operations seem to be near anyway
    \end{itemize}

    TODO:
    \begin{itemize}
        \item implement checks
        \item implement good storage of G
        \item implement efficient matmul of G
        \item implement one givens rotation
        \item implement outer loop
        \item serial optimizations
        \item parallelizations, probably with openmp
    \end{itemize}





\section{Experiments}
\section{Conclusion}
\section{References}
$https://www.math.usm.edu/lambers/mat610/sum10/lecture9.pdf
https://www.math.usm.edu/lambers/mat610/class0208.pdf
https://en.wikipedia.org/wiki/Givens_rotation$





\end{document}

